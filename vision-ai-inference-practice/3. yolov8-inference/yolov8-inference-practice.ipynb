{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from ultralytics import YOLO"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load a COCO-pretrained YOLOv8n model\n",
    "model = YOLO(\"yolov8n.pt\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "image 1/1 /Users/yunganglog/endpointTest/vision-ai-inference-practice/3. yolov8-inference/sample.jpg: 384x640 6 persons, 1 bicycle, 1 car, 2 buss, 1 truck, 2 traffic lights, 46.5ms\n",
      "Speed: 3.9ms preprocess, 46.5ms inference, 0.8ms postprocess per image at shape (1, 3, 384, 640)\n",
      "[ultralytics.engine.results.Results object with attributes:\n",
      "\n",
      "boxes: ultralytics.engine.results.Boxes object\n",
      "keypoints: None\n",
      "masks: None\n",
      "names: {0: 'person', 1: 'bicycle', 2: 'car', 3: 'motorcycle', 4: 'airplane', 5: 'bus', 6: 'train', 7: 'truck', 8: 'boat', 9: 'traffic light', 10: 'fire hydrant', 11: 'stop sign', 12: 'parking meter', 13: 'bench', 14: 'bird', 15: 'cat', 16: 'dog', 17: 'horse', 18: 'sheep', 19: 'cow', 20: 'elephant', 21: 'bear', 22: 'zebra', 23: 'giraffe', 24: 'backpack', 25: 'umbrella', 26: 'handbag', 27: 'tie', 28: 'suitcase', 29: 'frisbee', 30: 'skis', 31: 'snowboard', 32: 'sports ball', 33: 'kite', 34: 'baseball bat', 35: 'baseball glove', 36: 'skateboard', 37: 'surfboard', 38: 'tennis racket', 39: 'bottle', 40: 'wine glass', 41: 'cup', 42: 'fork', 43: 'knife', 44: 'spoon', 45: 'bowl', 46: 'banana', 47: 'apple', 48: 'sandwich', 49: 'orange', 50: 'broccoli', 51: 'carrot', 52: 'hot dog', 53: 'pizza', 54: 'donut', 55: 'cake', 56: 'chair', 57: 'couch', 58: 'potted plant', 59: 'bed', 60: 'dining table', 61: 'toilet', 62: 'tv', 63: 'laptop', 64: 'mouse', 65: 'remote', 66: 'keyboard', 67: 'cell phone', 68: 'microwave', 69: 'oven', 70: 'toaster', 71: 'sink', 72: 'refrigerator', 73: 'book', 74: 'clock', 75: 'vase', 76: 'scissors', 77: 'teddy bear', 78: 'hair drier', 79: 'toothbrush'}\n",
      "obb: None\n",
      "orig_img: array([[[ 62,  72,  56],\n",
      "        [ 47,  57,  41],\n",
      "        [ 46,  55,  42],\n",
      "        ...,\n",
      "        [ 88, 100, 110],\n",
      "        [ 91, 103, 113],\n",
      "        [ 95, 107, 117]],\n",
      "\n",
      "       [[ 69,  79,  63],\n",
      "        [ 54,  64,  48],\n",
      "        [ 55,  64,  51],\n",
      "        ...,\n",
      "        [ 92, 104, 114],\n",
      "        [ 95, 107, 117],\n",
      "        [ 94, 106, 116]],\n",
      "\n",
      "       [[ 63,  73,  57],\n",
      "        [ 53,  63,  47],\n",
      "        [ 57,  66,  53],\n",
      "        ...,\n",
      "        [100, 114, 126],\n",
      "        [105, 119, 131],\n",
      "        [104, 118, 130]],\n",
      "\n",
      "       ...,\n",
      "\n",
      "       [[138, 135, 131],\n",
      "        [139, 136, 132],\n",
      "        [140, 137, 133],\n",
      "        ...,\n",
      "        [140, 141, 137],\n",
      "        [142, 141, 137],\n",
      "        [142, 143, 139]],\n",
      "\n",
      "       [[141, 138, 134],\n",
      "        [141, 138, 134],\n",
      "        [140, 137, 133],\n",
      "        ...,\n",
      "        [100,  99,  95],\n",
      "        [101,  98,  94],\n",
      "        [105, 104, 100]],\n",
      "\n",
      "       [[143, 140, 136],\n",
      "        [141, 138, 134],\n",
      "        [140, 137, 133],\n",
      "        ...,\n",
      "        [121, 118, 114],\n",
      "        [131, 126, 123],\n",
      "        [147, 144, 140]]], dtype=uint8)\n",
      "orig_shape: (461, 768)\n",
      "path: '/Users/yunganglog/endpointTest/vision-ai-inference-practice/3. yolov8-inference/sample.jpg'\n",
      "probs: None\n",
      "save_dir: 'runs/detect/predict'\n",
      "speed: {'preprocess': 3.91387939453125, 'inference': 46.49686813354492, 'postprocess': 0.8158683776855469}]\n"
     ]
    }
   ],
   "source": [
    "# Run inference with the YOLOv8n model on the 'bus.jpg' image\n",
    "results = model(\"sample.jpg\")\n",
    "print(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ultralytics.engine.results.Boxes object with attributes:\n",
      "\n",
      "cls: tensor([5., 0., 0., 0., 0., 0., 5., 7., 9., 0., 9., 2., 1.])\n",
      "conf: tensor([0.8965, 0.8064, 0.7118, 0.7110, 0.6742, 0.6740, 0.5522, 0.4229, 0.3582, 0.3315, 0.3266, 0.3072, 0.3044])\n",
      "data: tensor([[1.5205e+02, 6.4378e+01, 3.7652e+02, 3.2296e+02, 8.9652e-01, 5.0000e+00],\n",
      "        [3.0697e+02, 2.3760e+02, 3.5670e+02, 3.5393e+02, 8.0640e-01, 0.0000e+00],\n",
      "        [4.8081e+01, 2.2395e+02, 7.8162e+01, 3.1741e+02, 7.1178e-01, 0.0000e+00],\n",
      "        [1.0281e+02, 2.1471e+02, 1.3778e+02, 3.4681e+02, 7.1104e-01, 0.0000e+00],\n",
      "        [1.9476e+01, 2.2288e+02, 4.4528e+01, 2.9273e+02, 6.7420e-01, 0.0000e+00],\n",
      "        [1.3551e+02, 2.1282e+02, 1.8331e+02, 3.6499e+02, 6.7404e-01, 0.0000e+00],\n",
      "        [3.6949e+02, 1.5184e+02, 5.0305e+02, 3.3001e+02, 5.5216e-01, 5.0000e+00],\n",
      "        [4.6323e+02, 1.6203e+02, 7.6748e+02, 3.7983e+02, 4.2294e-01, 7.0000e+00],\n",
      "        [1.0122e+02, 1.2236e+01, 1.4427e+02, 1.1749e+02, 3.5817e-01, 9.0000e+00],\n",
      "        [3.7065e+01, 2.2407e+02, 5.3971e+01, 2.8765e+02, 3.3146e-01, 0.0000e+00],\n",
      "        [1.0902e+02, 1.2416e+02, 1.2165e+02, 1.5641e+02, 3.2663e-01, 9.0000e+00],\n",
      "        [4.6348e+02, 1.6236e+02, 7.6743e+02, 3.7991e+02, 3.0721e-01, 2.0000e+00],\n",
      "        [3.1627e+02, 2.8468e+02, 3.5688e+02, 3.5906e+02, 3.0443e-01, 1.0000e+00]])\n",
      "id: None\n",
      "is_track: False\n",
      "orig_shape: (461, 768)\n",
      "shape: torch.Size([13, 6])\n",
      "xywh: tensor([[264.2817, 193.6684, 224.4714, 258.5807],\n",
      "        [331.8348, 295.7624,  49.7296, 116.3329],\n",
      "        [ 63.1216, 270.6781,  30.0808,  93.4635],\n",
      "        [120.2975, 280.7583,  34.9704, 132.1022],\n",
      "        [ 32.0022, 257.8053,  25.0525,  69.8538],\n",
      "        [159.4118, 288.9022,  47.8007, 152.1664],\n",
      "        [436.2697, 240.9244, 133.5617, 178.1700],\n",
      "        [615.3566, 270.9323, 304.2494, 217.7977],\n",
      "        [122.7449,  64.8616,  43.0573, 105.2516],\n",
      "        [ 45.5179, 255.8593,  16.9063,  63.5830],\n",
      "        [115.3360, 140.2839,  12.6350,  32.2532],\n",
      "        [615.4582, 271.1383, 303.9480, 217.5526],\n",
      "        [336.5759, 321.8665,  40.6097,  74.3797]])\n",
      "xywhn: tensor([[0.3441, 0.4201, 0.2923, 0.5609],\n",
      "        [0.4321, 0.6416, 0.0648, 0.2523],\n",
      "        [0.0822, 0.5872, 0.0392, 0.2027],\n",
      "        [0.1566, 0.6090, 0.0455, 0.2866],\n",
      "        [0.0417, 0.5592, 0.0326, 0.1515],\n",
      "        [0.2076, 0.6267, 0.0622, 0.3301],\n",
      "        [0.5681, 0.5226, 0.1739, 0.3865],\n",
      "        [0.8012, 0.5877, 0.3962, 0.4724],\n",
      "        [0.1598, 0.1407, 0.0561, 0.2283],\n",
      "        [0.0593, 0.5550, 0.0220, 0.1379],\n",
      "        [0.1502, 0.3043, 0.0165, 0.0700],\n",
      "        [0.8014, 0.5882, 0.3958, 0.4719],\n",
      "        [0.4382, 0.6982, 0.0529, 0.1613]])\n",
      "xyxy: tensor([[152.0460,  64.3780, 376.5174, 322.9587],\n",
      "        [306.9700, 237.5959, 356.6996, 353.9288],\n",
      "        [ 48.0812, 223.9464,  78.1620, 317.4099],\n",
      "        [102.8124, 214.7072, 137.7827, 346.8094],\n",
      "        [ 19.4760, 222.8784,  44.5285, 292.7321],\n",
      "        [135.5114, 212.8190, 183.3121, 364.9854],\n",
      "        [369.4888, 151.8394, 503.0505, 330.0094],\n",
      "        [463.2320, 162.0335, 767.4813, 379.8312],\n",
      "        [101.2162,  12.2359, 144.2735, 117.4874],\n",
      "        [ 37.0647, 224.0678,  53.9710, 287.6508],\n",
      "        [109.0184, 124.1573, 121.6535, 156.4105],\n",
      "        [463.4842, 162.3620, 767.4322, 379.9146],\n",
      "        [316.2710, 284.6766, 356.8807, 359.0563]])\n",
      "xyxyn: tensor([[0.1980, 0.1396, 0.4903, 0.7006],\n",
      "        [0.3997, 0.5154, 0.4645, 0.7677],\n",
      "        [0.0626, 0.4858, 0.1018, 0.6885],\n",
      "        [0.1339, 0.4657, 0.1794, 0.7523],\n",
      "        [0.0254, 0.4835, 0.0580, 0.6350],\n",
      "        [0.1764, 0.4616, 0.2387, 0.7917],\n",
      "        [0.4811, 0.3294, 0.6550, 0.7159],\n",
      "        [0.6032, 0.3515, 0.9993, 0.8239],\n",
      "        [0.1318, 0.0265, 0.1879, 0.2549],\n",
      "        [0.0483, 0.4860, 0.0703, 0.6240],\n",
      "        [0.1420, 0.2693, 0.1584, 0.3393],\n",
      "        [0.6035, 0.3522, 0.9993, 0.8241],\n",
      "        [0.4118, 0.6175, 0.4647, 0.7789]])\n"
     ]
    }
   ],
   "source": [
    "# Process results list\n",
    "for result in results:\n",
    "    boxes = result.boxes  # Boxes object for bounding box outputs\n",
    "    print(boxes)\n",
    "    masks = result.masks  # Masks object for segmentation masks outputs\n",
    "    keypoints = result.keypoints  # Keypoints object for pose outputs\n",
    "    probs = result.probs  # Probs object for classification outputs\n",
    "    obb = result.obb  # Oriented boxes object for OBB outputs\n",
    "    result.show()  # display to screen\n",
    "    result.save(filename=\"result.jpg\")  # save to disk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.5"
  },
  "vscode": {
   "interpreter": {
    "hash": "40d3a090f54c6569ab1632332b64b2c03c39dcf918b08424e98f38b5ae0af88f"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
